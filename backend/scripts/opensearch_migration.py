#!/usr/bin/env python3
"""
MySQL ë°ì´í„°ë¥¼ OpenSearchë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
"""

import sys
import os
from pathlib import Path

# ë°±ì—”ë“œ ì•± ëª¨ë“ˆì„ importí•˜ê¸° ìœ„í•´ ê²½ë¡œ ì¶”ê°€
backend_path = Path(__file__).parent.parent
sys.path.insert(0, str(backend_path))

import asyncio
import mysql.connector
from datetime import datetime
from typing import List, Dict, Any, Optional
import json
from loguru import logger
from opensearchpy import OpenSearch
from app.core.config import settings
from app.core.opensearch_client import get_opensearch_client


class OpenSearchMigration:
    """OpenSearch ë§ˆì´ê·¸ë ˆì´ì…˜ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.opensearch_client = get_opensearch_client()
        self.mysql_config = {
            'host': settings.MYSQL_SERVER,
            'user': settings.MYSQL_USER,
            'password': settings.MYSQL_PASSWORD,
            'database': settings.MYSQL_DB,
            'port': int(settings.MYSQL_PORT),
            'charset': 'utf8mb4'
        }
        
    def get_products_index_mapping(self) -> Dict[str, Any]:
        """ìƒí’ˆ ì¸ë±ìŠ¤ ë§¤í•‘ ì •ì˜"""
        return {
            "settings": {
                "number_of_shards": 1,
                "number_of_replicas": 0,
                "analysis": {
                    "analyzer": {
                        "korean_analyzer": {
                            "type": "standard"
                        }
                    }
                }
            },
            "mappings": {
                "properties": {
                    "product_no": {"type": "integer"},
                    "product_id": {"type": "keyword"},
                    "product_name": {
                        "type": "text",
                        "analyzer": "korean_analyzer",
                        "fields": {
                            "keyword": {"type": "keyword"},
                            "ngram": {
                                "type": "text",
                                "analyzer": "standard"
                            }
                        }
                    },
                    "category": {
                        "type": "object",
                        "properties": {
                            "category_id": {"type": "integer"},
                            "category_name": {
                                "type": "text",
                                "analyzer": "korean_analyzer",
                                "fields": {"keyword": {"type": "keyword"}}
                            },
                            "category_code": {"type": "keyword"},
                            "parent_category_id": {"type": "integer"},
                            "depth": {"type": "integer"}
                        }
                    },
                    "brand": {
                        "type": "text",
                        "analyzer": "korean_analyzer",
                        "fields": {"keyword": {"type": "keyword"}}
                    },
                    "price": {"type": "double"},
                    "description": {
                        "type": "text",
                        "analyzer": "korean_analyzer"
                    },
                    "image_url": {"type": "keyword"},
                    "statistics": {
                        "type": "object",
                        "properties": {
                            "total_reviews": {"type": "integer"},
                            "average_rating": {"type": "float"},
                            "rating_distribution": {"type": "object"},
                            "last_review_date": {"type": "date"},
                            "review_velocity": {"type": "float"}
                        }
                    },
                    "created_at": {"type": "date"},
                    "updated_at": {"type": "date"},
                    "suggest": {
                        "type": "completion",
                        "analyzer": "korean_analyzer"
                    }
                }
            }
        }
    
    def get_reviews_index_mapping(self) -> Dict[str, Any]:
        """ë¦¬ë·° ì¸ë±ìŠ¤ ë§¤í•‘ ì •ì˜"""
        return {
            "settings": {
                "number_of_shards": 1,
                "number_of_replicas": 0,
                "analysis": {
                    "analyzer": {
                        "korean_analyzer": {
                            "type": "standard"
                        }
                    }
                }
            },
            "mappings": {
                "properties": {
                    "review_id": {"type": "integer"},
                    "product_no": {"type": "integer"},
                    "member_no": {"type": "integer"},
                    "product_name": {
                        "type": "text",
                        "analyzer": "korean_analyzer"
                    },
                    "rating": {"type": "integer"},
                    "review_text": {
                        "type": "text",
                        "analyzer": "korean_analyzer"
                    },
                    "review_date": {"type": "date"},
                    "helpful_count": {"type": "integer"},
                    "sentiment": {"type": "keyword"},  # positive, negative, neutral
                    "review_embedding": {  # Vertex AI text-multilingual-embedding-002 ë²¡í„°
                        "type": "knn_vector",
                        "dimension": 768,
                        "method": {
                            "name": "hnsw",
                            "space_type": "cosinesimil",
                            "engine": "nmslib",
                            "parameters": {
                                "ef_construction": 512,
                                "m": 16
                            }
                        }
                    },
                    "created_at": {"type": "date"},
                    "updated_at": {"type": "date"}
                }
            }
        }
    
    def connect_mysql(self) -> mysql.connector.MySQLConnection:
        """MySQL ì—°ê²°"""
        try:
            connection = mysql.connector.connect(**self.mysql_config)
            logger.info("MySQL ì—°ê²° ì„±ê³µ")
            return connection
        except Exception as e:
            logger.error(f"MySQL ì—°ê²° ì‹¤íŒ¨: {e}")
            raise
    
    def fetch_products_with_category(self, connection: mysql.connector.MySQLConnection, 
                                   batch_size: int = 1000, offset: int = 0) -> List[Dict[str, Any]]:
        """ìƒí’ˆ ë°ì´í„°ë¥¼ ì¹´í…Œê³ ë¦¬ ì •ë³´ì™€ í•¨ê»˜ ì¡°íšŒ"""
        query = """
        SELECT 
            p.product_no,
            p.product_id,
            p.product_name,
            p.brand,
            p.price,
            p.description,
            p.image_url,
            p.created_at,
            p.updated_at,
            c.category_id,
            c.category_name,
            c.category_code,
            c.parent_category_id,
            c.depth,
            ps.total_reviews,
            ps.average_rating,
            ps.rating_distribution,
            ps.last_review_date,
            ps.review_velocity
        FROM products p
        LEFT JOIN categories c ON p.category_id = c.category_id
        LEFT JOIN product_statistics ps ON p.product_no = ps.product_no
        ORDER BY p.product_no
        LIMIT %s OFFSET %s
        """
        
        cursor = connection.cursor(dictionary=True)
        cursor.execute(query, (batch_size, offset))
        results = cursor.fetchall()
        cursor.close()
        
        # ë°ì´í„° ë³€í™˜
        products = []
        for row in results:
            # rating_distribution JSON íŒŒì‹±
            rating_distribution = None
            if row['rating_distribution']:
                try:
                    rating_distribution = json.loads(row['rating_distribution'])
                except:
                    rating_distribution = None
            
            product = {
                "product_no": row['product_no'],
                "product_id": row['product_id'],
                "product_name": row['product_name'],
                "brand": row['brand'],
                "price": float(row['price']) if row['price'] else 0.0,
                "description": row['description'] or "",
                "image_url": row['image_url'],
                "category": {
                    "category_id": row['category_id'],
                    "category_name": row['category_name'],
                    "category_code": row['category_code'],
                    "parent_category_id": row['parent_category_id'],
                    "depth": row['depth']
                } if row['category_id'] else None,
                "statistics": {
                    "total_reviews": row['total_reviews'] or 0,
                    "average_rating": float(row['average_rating']) if row['average_rating'] else 0.0,
                    "rating_distribution": rating_distribution,
                    "last_review_date": row['last_review_date'].isoformat() if row['last_review_date'] else None,
                    "review_velocity": float(row['review_velocity']) if row['review_velocity'] else 0.0
                },
                "created_at": row['created_at'].isoformat() if row['created_at'] else None,
                "updated_at": row['updated_at'].isoformat() if row['updated_at'] else None,
                # ìë™ì™„ì„±ì„ ìœ„í•œ suggest í•„ë“œ
                "suggest": {
                    "input": [
                        row['product_name'],
                        row['brand'] if row['brand'] else "",
                        row['category_name'] if row['category_name'] else ""
                    ],
                    "weight": row['total_reviews'] or 1 if row['total_reviews'] else 1
                }
            }
            products.append(product)
        
        return products
    
    def fetch_reviews_with_product(self, connection: mysql.connector.MySQLConnection,
                                 batch_size: int = 1000, offset: int = 0) -> List[Dict[str, Any]]:
        """ë¦¬ë·° ë°ì´í„°ë¥¼ ìƒí’ˆ ì •ë³´ì™€ í•¨ê»˜ ì¡°íšŒ"""
        query = """
        SELECT 
            r.review_id,
            r.product_no,
            r.member_no,
            r.rating,
            r.review_text,
            r.review_date,
            r.helpful_count,
            r.created_at,
            r.updated_at,
            p.product_name
        FROM reviews r
        JOIN products p ON r.product_no = p.product_no
        ORDER BY r.review_id
        LIMIT %s OFFSET %s
        """
        
        cursor = connection.cursor(dictionary=True)
        cursor.execute(query, (batch_size, offset))
        results = cursor.fetchall()
        cursor.close()
        
        # ë°ì´í„° ë³€í™˜
        reviews = []
        for row in results:
            # ê°„ë‹¨í•œ ê°ì • ë¶„ì„ (í‚¤ì›Œë“œ ê¸°ë°˜)
            sentiment = self.analyze_sentiment(row['review_text'])
            
            review = {
                "review_id": row['review_id'],
                "product_no": row['product_no'],
                "member_no": row['member_no'],
                "product_name": row['product_name'],
                "rating": row['rating'],
                "review_text": row['review_text'] or "",
                "review_date": row['review_date'].isoformat() if row['review_date'] else None,
                "helpful_count": row['helpful_count'] or 0,
                "sentiment": sentiment,
                "created_at": row['created_at'].isoformat() if row['created_at'] else None,
                "updated_at": row['updated_at'].isoformat() if row['updated_at'] else None
            }
            reviews.append(review)
        
        return reviews
    
    def analyze_sentiment(self, text: str) -> str:
        """ê°„ë‹¨í•œ ê°ì • ë¶„ì„"""
        if not text:
            return "neutral"
        
        positive_words = ["ì¢‹", "ë§Œì¡±", "ì¶”ì²œ", "í›Œë¥­", "ì™„ë²½", "ìµœê³ ", "ê°ì‚¬", "í–‰ë³µ"]
        negative_words = ["ë‚˜ì˜", "ì‹¤ë§", "ë¶ˆë§Œ", "ë³„ë¡œ", "ì•„ì‰½", "í›„íšŒ", "ìµœì•…", "ì§œì¦"]
        
        text = text.lower()
        positive_count = sum(1 for word in positive_words if word in text)
        negative_count = sum(1 for word in negative_words if word in text)
        
        if positive_count > negative_count:
            return "positive"
        elif negative_count > positive_count:
            return "negative"
        else:
            return "neutral"
    
    def get_total_count(self, connection: mysql.connector.MySQLConnection, table: str) -> int:
        """í…Œì´ë¸”ì˜ ì´ ë ˆì½”ë“œ ìˆ˜ ì¡°íšŒ"""
        cursor = connection.cursor()
        cursor.execute(f"SELECT COUNT(*) FROM {table}")
        count = cursor.fetchone()[0]
        cursor.close()
        return count
    
    def migrate_products(self, batch_size: int = 1000) -> bool:
        """ìƒí’ˆ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜"""
        logger.info("ìƒí’ˆ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘")
        
        # ì¸ë±ìŠ¤ ìƒì„±
        products_mapping = self.get_products_index_mapping()
        index_name = "products"
        
        if not self.opensearch_client.create_index(index_name, products_mapping):
            logger.error("ìƒí’ˆ ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨")
            return False
        
        # MySQL ì—°ê²°
        connection = self.connect_mysql()
        
        try:
            # ì´ ë ˆì½”ë“œ ìˆ˜ í™•ì¸
            total_products = self.get_total_count(connection, "products")
            logger.info(f"ì´ ìƒí’ˆ ìˆ˜: {total_products:,}ê°œ")
            
            # ë°°ì¹˜ ì²˜ë¦¬
            offset = 0
            processed = 0
            
            while offset < total_products:
                # ë°ì´í„° ì¡°íšŒ
                products = self.fetch_products_with_category(connection, batch_size, offset)
                
                if not products:
                    break
                
                # OpenSearchì— bulk ì¸ë±ì‹±
                if self.opensearch_client.bulk_index(index_name, products):
                    processed += len(products)
                    logger.info(f"ìƒí’ˆ ë°ì´í„° ì²˜ë¦¬: {processed:,}/{total_products:,} ({processed/total_products*100:.1f}%)")
                else:
                    logger.error(f"ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: offset {offset}")
                
                offset += batch_size
            
            logger.info(f"ìƒí’ˆ ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ: {processed:,}ê°œ")
            return True
            
        except Exception as e:
            logger.error(f"ìƒí’ˆ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨: {e}")
            return False
        finally:
            connection.close()
    
    def migrate_reviews(self, batch_size: int = 1000) -> bool:
        """ë¦¬ë·° ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜"""
        logger.info("ë¦¬ë·° ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘")
        
        # ì¸ë±ìŠ¤ ìƒì„±
        reviews_mapping = self.get_reviews_index_mapping()
        index_name = "reviews"
        
        if not self.opensearch_client.create_index(index_name, reviews_mapping):
            logger.error("ë¦¬ë·° ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨")
            return False
        
        # MySQL ì—°ê²°
        connection = self.connect_mysql()
        
        try:
            # ì´ ë ˆì½”ë“œ ìˆ˜ í™•ì¸
            total_reviews = self.get_total_count(connection, "reviews")
            logger.info(f"ì´ ë¦¬ë·° ìˆ˜: {total_reviews:,}ê°œ")
            
            # ë°°ì¹˜ ì²˜ë¦¬
            offset = 0
            processed = 0
            
            while offset < total_reviews:
                # ë°ì´í„° ì¡°íšŒ
                reviews = self.fetch_reviews_with_product(connection, batch_size, offset)
                
                if not reviews:
                    break
                
                # OpenSearchì— bulk ì¸ë±ì‹±
                if self.opensearch_client.bulk_index(index_name, reviews):
                    processed += len(reviews)
                    logger.info(f"ë¦¬ë·° ë°ì´í„° ì²˜ë¦¬: {processed:,}/{total_reviews:,} ({processed/total_reviews*100:.1f}%)")
                else:
                    logger.error(f"ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: offset {offset}")
                
                offset += batch_size
            
            logger.info(f"ë¦¬ë·° ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ: {processed:,}ê°œ")
            return True
            
        except Exception as e:
            logger.error(f"ë¦¬ë·° ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨: {e}")
            return False
        finally:
            connection.close()
    
    def test_search(self) -> bool:
        """ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"""
        logger.info("ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        try:
            # ìƒí’ˆ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
            search_query = {
                "query": {
                    "multi_match": {
                        "query": "ì‚¼ì„±",
                        "fields": ["product_name", "brand", "description"]
                    }
                },
                "size": 5
            }
            
            results = self.opensearch_client.search("products", search_query)
            logger.info(f"ìƒí’ˆ ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ")
            
            for result in results[:3]:
                logger.info(f"  - {result.get('product_name')} ({result.get('brand')})")
            
            # ë¦¬ë·° ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
            review_query = {
                "query": {
                    "match": {
                        "review_text": "ì¢‹ì•„ìš”"
                    }
                },
                "size": 3
            }
            
            review_results = self.opensearch_client.search("reviews", review_query)
            logger.info(f"ë¦¬ë·° ê²€ìƒ‰ ê²°ê³¼: {len(review_results)}ê°œ")
            
            return True
            
        except Exception as e:
            logger.error(f"ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return False
    
    def run_migration(self, migrate_products: bool = True, migrate_reviews: bool = True, 
                     batch_size: int = 1000) -> bool:
        """ì „ì²´ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰"""
        logger.info("ğŸš€ OpenSearch ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘")
        
        # OpenSearch ì—°ê²° í™•ì¸
        if not self.opensearch_client.is_connected():
            logger.error("OpenSearch ì—°ê²° ì‹¤íŒ¨")
            return False
        
        success = True
        
        # ìƒí’ˆ ë§ˆì´ê·¸ë ˆì´ì…˜
        if migrate_products:
            if not self.migrate_products(batch_size):
                success = False
        
        # ë¦¬ë·° ë§ˆì´ê·¸ë ˆì´ì…˜
        if migrate_reviews:
            if not self.migrate_reviews(batch_size):
                success = False
        
        # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        if success:
            self.test_search()
        
        if success:
            logger.info("ğŸ‰ OpenSearch ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ!")
        else:
            logger.error("âŒ ë§ˆì´ê·¸ë ˆì´ì…˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
        
        return success


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="MySQL to OpenSearch ë§ˆì´ê·¸ë ˆì´ì…˜")
    parser.add_argument("--products-only", action="store_true", help="ìƒí’ˆ ë°ì´í„°ë§Œ ë§ˆì´ê·¸ë ˆì´ì…˜")
    parser.add_argument("--reviews-only", action="store_true", help="ë¦¬ë·° ë°ì´í„°ë§Œ ë§ˆì´ê·¸ë ˆì´ì…˜")
    parser.add_argument("--batch-size", type=int, default=1000, help="ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 1000)")
    
    args = parser.parse_args()
    
    migration = OpenSearchMigration()
    
    migrate_products = not args.reviews_only
    migrate_reviews = not args.products_only
    
    success = migration.run_migration(
        migrate_products=migrate_products,
        migrate_reviews=migrate_reviews,
        batch_size=args.batch_size
    )
    
    return 0 if success else 1


if __name__ == "__main__":
    exit(main()) 