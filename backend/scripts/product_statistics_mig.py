#!/usr/bin/env python3
"""
ìƒí’ˆ í†µê³„ ë§ˆì´ê·¸ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸
- ë¦¬ë·° ë°ì´í„°ì—ì„œ ìƒí’ˆë³„ í†µê³„ ì§‘ê³„
- product_statistics.csv ìƒì„±
"""

import pandas as pd
import json
from pathlib import Path
import logging
from datetime import datetime, timedelta
from collections import defaultdict

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ProductStatisticsMigration:
    def __init__(self):
        self.product_statistics = []
        self.products_info = {}  # product_no -> product info
        
    def load_products_info(self, products_file="../../data/mysql_ready/products.csv"):
        """ìƒí’ˆ ì •ë³´ ë¡œë“œ"""
        logger.info("ğŸ“¦ ìƒí’ˆ ì •ë³´ ë¡œë“œ")
        
        try:
            df_products = pd.read_csv(products_file)
            
            for _, row in df_products.iterrows():
                product_no = row['product_no']
                self.products_info[product_no] = {
                    'product_id': row.get('product_id'),
                    'product_name': row.get('product_name'),
                    'brand': row.get('brand')
                }
            
            logger.info(f"âœ… ìƒí’ˆ ì •ë³´ ë¡œë“œ ì™„ë£Œ: {len(self.products_info):,}ê°œ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ìƒí’ˆ ì •ë³´ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def calculate_review_velocity(self, reviews_group):
        """ì›”í‰ê·  ë¦¬ë·° ìˆ˜ ê³„ì‚°"""
        try:
            review_dates = pd.to_datetime(reviews_group['review_date'])
            if len(review_dates) < 2:
                return 0.0
            
            first_date = review_dates.min()
            last_date = review_dates.max()
            
            # ê¸°ê°„ ê³„ì‚° (ì›” ë‹¨ìœ„)
            date_diff = last_date - first_date
            months = max(1, date_diff.days / 30.44)  # í‰ê·  ì›” ì¼ìˆ˜
            
            # ì›”í‰ê·  ë¦¬ë·° ìˆ˜
            velocity = len(reviews_group) / months
            return round(velocity, 2)
            
        except Exception as e:
            logger.warning(f"âš ï¸ ë¦¬ë·° ì†ë„ ê³„ì‚° ì˜¤ë¥˜: {e}")
            return 0.0
    
    def analyze_reviews_data(self, df):
        """ë¦¬ë·° ë°ì´í„° ë¶„ì„"""
        logger.info("ğŸ” ë¦¬ë·° ë°ì´í„° ë¶„ì„")
        
        total_reviews = len(df)
        logger.info(f"ğŸ“Š ì´ ë¦¬ë·° ìˆ˜: {total_reviews:,}ê°œ")
        
        # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
        required_cols = ['product_no', 'rating', 'review_date']
        missing_cols = [col for col in required_cols if col not in df.columns]
        
        if missing_cols:
            logger.error(f"âŒ í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing_cols}")
            return False
        
        # ìœ íš¨í•œ ë°ì´í„° í™•ì¸
        valid_product_no = df['product_no'].notna().sum()
        valid_rating = df['rating'].between(1, 5).sum()
        valid_date = df['review_date'].notna().sum()
        
        logger.info(f"ğŸ¯ ìœ íš¨í•œ product_no: {valid_product_no:,}ê°œ ({valid_product_no/total_reviews*100:.1f}%)")
        logger.info(f"ğŸ¯ ìœ íš¨í•œ rating: {valid_rating:,}ê°œ ({valid_rating/total_reviews*100:.1f}%)")
        logger.info(f"ğŸ¯ ìœ íš¨í•œ review_date: {valid_date:,}ê°œ ({valid_date/total_reviews*100:.1f}%)")
        
        # ê³ ìœ  ìƒí’ˆ ìˆ˜
        unique_products = df['product_no'].nunique()
        logger.info(f"ğŸ›ï¸ ë¦¬ë·°ê°€ ìˆëŠ” ìƒí’ˆ: {unique_products:,}ê°œ")
        
        return True
    
    def process_product_statistics(self, df):
        """ìƒí’ˆë³„ í†µê³„ ì²˜ë¦¬"""
        logger.info("ğŸ“Š ìƒí’ˆë³„ í†µê³„ ì²˜ë¦¬ ì‹œì‘")
        
        # ìœ íš¨í•œ ë°ì´í„°ë§Œ í•„í„°ë§
        df_valid = df.dropna(subset=['product_no', 'rating', 'review_date']).copy()
        df_valid = df_valid[df_valid['rating'].between(1, 5)]
        
        logger.info(f"ğŸ“Š ìœ íš¨í•œ ë¦¬ë·° ë°ì´í„°: {len(df_valid):,}ê°œ")
        
        # ìƒí’ˆë³„ë¡œ ê·¸ë£¹í™”
        product_groups = df_valid.groupby('product_no')
        
        processed_count = 0
        skipped_count = 0
        
        for product_no, group in product_groups:
            try:
                # ê¸°ë³¸ í†µê³„
                total_reviews = len(group)
                average_rating = group['rating'].mean()
                
                # í‰ì  ë¶„í¬ ê³„ì‚° (JSON í˜•íƒœ)
                rating_counts = group['rating'].value_counts()
                rating_distribution = {}
                for rating in range(1, 6):
                    rating_distribution[str(rating)] = int(rating_counts.get(rating, 0))
                
                # ë‚ ì§œ ì •ë³´
                review_dates = pd.to_datetime(group['review_date'])
                last_review_date = review_dates.max().strftime('%Y-%m-%d')
                
                # ì›”í‰ê·  ë¦¬ë·° ìˆ˜
                review_velocity = self.calculate_review_velocity(group)
                
                # ìƒí’ˆ í†µê³„ ë°ì´í„° ìƒì„±
                product_stat = {
                    'product_no': int(product_no),
                    'total_reviews': total_reviews,
                    'average_rating': round(average_rating, 2),
                    'rating_distribution': json.dumps(rating_distribution),
                    'last_review_date': last_review_date,
                    'review_velocity': review_velocity
                }
                
                self.product_statistics.append(product_stat)
                processed_count += 1
                
                # ì§„í–‰ìƒí™© í‘œì‹œ
                if processed_count % 100 == 0:
                    logger.info(f"   ì²˜ë¦¬ ì§„í–‰: {processed_count:,}ê°œ ìƒí’ˆ ì™„ë£Œ...")
                
            except Exception as e:
                logger.warning(f"âš ï¸ ìƒí’ˆ í†µê³„ ì²˜ë¦¬ ì˜¤ë¥˜ (product_no: {product_no}): {e}")
                skipped_count += 1
                continue
        
        logger.info(f"âœ… ìƒí’ˆ í†µê³„ ì²˜ë¦¬ ì™„ë£Œ: {processed_count:,}ê°œ ì²˜ë¦¬, {skipped_count:,}ê°œ ìŠ¤í‚µ")
        
        # ì²˜ë¦¬ ì„±ê³µë¥ 
        total_products = len(product_groups)
        success_rate = (processed_count / total_products) * 100 if total_products > 0 else 0
        logger.info(f"ğŸ“Š ì²˜ë¦¬ ì„±ê³µë¥ : {success_rate:.1f}% ({processed_count}/{total_products})")
    
    def save_csv_file(self, output_dir="../../data/mysql_ready"):
        """CSV íŒŒì¼ ì €ì¥"""
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # Product Statistics CSV
        if self.product_statistics:
            df_stats = pd.DataFrame(self.product_statistics)
            stats_file = output_path / 'product_statistics.csv'
            df_stats.to_csv(stats_file, index=False, encoding='utf-8-sig')
            logger.info(f"âœ… {stats_file} ì €ì¥ ì™„ë£Œ: {len(self.product_statistics):,}ê°œ")
        
        logger.info(f"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_path.resolve()}")
    
    def run(self, reviews_file=None):
        """ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        logger.info("ğŸš€ ìƒí’ˆ í†µê³„ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘")
        
        # ì…ë ¥ íŒŒì¼ ì°¾ê¸°
        if not reviews_file:
            reviews_file = "../../data/mysql_ready/reviews.csv"
            
            if not Path(reviews_file).exists():
                logger.error(f"âŒ ë¦¬ë·° ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {reviews_file}")
                return False
        
        # ìƒí’ˆ ì •ë³´ ë¡œë“œ
        if not self.load_products_info():
            return False
        
        # ë¦¬ë·° ë°ì´í„° ë¡œë“œ
        logger.info(f"ğŸ“ ë¦¬ë·° ë°ì´í„° ë¡œë“œ: {reviews_file}")
        try:
            df = pd.read_csv(reviews_file)
            logger.info(f"ğŸ“Š ë¡œë“œëœ ë°ì´í„°: {len(df):,}í–‰ x {len(df.columns):,}ì—´")
        except Exception as e:
            logger.error(f"âŒ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
        
        # ë°ì´í„° ë¶„ì„ ë° ì²˜ë¦¬
        if not self.analyze_reviews_data(df):
            return False
        
        self.process_product_statistics(df)
        
        # CSV ì €ì¥
        self.save_csv_file()
        
        # ìš”ì•½ ì •ë³´
        logger.info("=" * 60)
        logger.info("ğŸ“‹ ìƒí’ˆ í†µê³„ ë§ˆì´ê·¸ë ˆì´ì…˜ ìš”ì•½")
        logger.info(f"ğŸ“Š ì›ë³¸ ë¦¬ë·° ë°ì´í„°: {len(df):,}í–‰")
        logger.info(f"ğŸ“ˆ ìƒì„±ëœ ìƒí’ˆ í†µê³„: {len(self.product_statistics):,}ê°œ")
        
        # í†µê³„ ì •ë³´
        if self.product_statistics:
            # ë¦¬ë·° ìˆ˜ í†µê³„
            review_counts = [stat['total_reviews'] for stat in self.product_statistics]
            avg_reviews = sum(review_counts) / len(review_counts)
            max_reviews = max(review_counts)
            min_reviews = min(review_counts)
            
            logger.info(f"ğŸ“Š ìƒí’ˆë³„ ë¦¬ë·° ìˆ˜ í†µê³„:")
            logger.info(f"   í‰ê·  ë¦¬ë·° ìˆ˜: {avg_reviews:.1f}ê°œ")
            logger.info(f"   ìµœëŒ€ ë¦¬ë·° ìˆ˜: {max_reviews}ê°œ")
            logger.info(f"   ìµœì†Œ ë¦¬ë·° ìˆ˜: {min_reviews}ê°œ")
            
            # í‰ì  í†µê³„
            ratings = [stat['average_rating'] for stat in self.product_statistics]
            avg_rating = sum(ratings) / len(ratings)
            max_rating = max(ratings)
            min_rating = min(ratings)
            
            logger.info(f"â­ ìƒí’ˆë³„ í‰ì  í†µê³„:")
            logger.info(f"   ì „ì²´ í‰ê·  í‰ì : {avg_rating:.2f}ì ")
            logger.info(f"   ìµœê³  í‰ì : {max_rating:.2f}ì ")
            logger.info(f"   ìµœì € í‰ì : {min_rating:.2f}ì ")
            
            # ë¦¬ë·° ì†ë„ í†µê³„
            velocities = [stat['review_velocity'] for stat in self.product_statistics if stat['review_velocity'] > 0]
            if velocities:
                avg_velocity = sum(velocities) / len(velocities)
                max_velocity = max(velocities)
                logger.info(f"ğŸš€ ë¦¬ë·° ì†ë„ í†µê³„:")
                logger.info(f"   í‰ê·  ì›” ë¦¬ë·° ìˆ˜: {avg_velocity:.1f}ê°œ")
                logger.info(f"   ìµœëŒ€ ì›” ë¦¬ë·° ìˆ˜: {max_velocity:.1f}ê°œ")
            
            # í‰ì  ë¶„í¬ ìƒ˜í”Œ ì¶œë ¥
            logger.info("ğŸ“Š í‰ì  ë¶„í¬ ìƒ˜í”Œ (ìƒìœ„ 3ê°œ ìƒí’ˆ):")
            top_products = sorted(self.product_statistics, key=lambda x: x['total_reviews'], reverse=True)[:3]
            for i, product in enumerate(top_products, 1):
                rating_dist = json.loads(product['rating_distribution'])
                dist_str = ", ".join([f"{k}ì :{v}ê°œ" for k, v in rating_dist.items() if v > 0])
                logger.info(f"   {i}. ìƒí’ˆ {product['product_no']} ({product['total_reviews']}ê°œ ë¦¬ë·°): {dist_str}")
        
        logger.info("=" * 60)
        logger.info("ğŸ‰ ìƒí’ˆ í†µê³„ ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ!")
        return True

def main():
    migration = ProductStatisticsMigration()
    success = migration.run()
    return success

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1) 