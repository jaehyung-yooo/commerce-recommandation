#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ìƒí’ˆ ë¦¬ë·° í¬ë¡¤ëŸ¬

ê¸°ëŠ¥:
- SSF Shop ìƒí’ˆ ë¦¬ë·° í¬ë¡¤ë§
- ë¦¬ë·°ê°€ ì—†ì„ ë•Œê¹Œì§€ ëª¨ë“  í˜ì´ì§€ ìë™ í¬ë¡¤ë§
- ì¼ì • ê°œìˆ˜ë§ˆë‹¤ ì¤‘ê°„ì €ì¥ (ê¸°ë³¸: 100ê°œ)
- ë‹¨ì¼ íŒŒì¼ì— ì‹¤ì‹œê°„ ì €ì¥
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì  í¬ë¡¤ë§

ì‚¬ìš©ë²•:
  python review_crawler.py                    # ê¸°ë³¸ ì‹¤í–‰ (100ê°œë§ˆë‹¤ ì €ì¥, ëª¨ë“  í˜ì´ì§€)
  python review_crawler.py --batch 50        # 50ê°œë§ˆë‹¤ ì €ì¥
  python review_crawler.py -b 200           # 200ê°œë§ˆë‹¤ ì €ì¥
  python review_crawler.py --batch=75       # 75ê°œë§ˆë‹¤ ì €ì¥
  python review_crawler.py 100              # í…ŒìŠ¤íŠ¸: 100ê°œ ìƒí’ˆë§Œ
  python review_crawler.py products.csv     # íŠ¹ì • íŒŒì¼ ì‚¬ìš©

ì£¼ìš” íŠ¹ì§•:
- ë¦¬ë·°ê°€ ìˆëŠ” ë§Œí¼ ëª¨ë“  í˜ì´ì§€ë¥¼ ìë™ìœ¼ë¡œ í¬ë¡¤ë§
- í˜ì´ì§€ ìˆ˜ ì œí•œ ì—†ìŒ (ë¦¬ë·°ê°€ ì—†ìœ¼ë©´ ìë™ ì¤‘ë‹¨)
- ìµœëŒ€ 50í˜ì´ì§€ ì•ˆì „ì¥ì¹˜ (ë¬´í•œ ë£¨í”„ ë°©ì§€)
"""

import pandas as pd
import requests
from bs4 import BeautifulSoup
import time
from datetime import datetime
import sys
import re
from urllib.parse import parse_qs, urlparse


class ReviewCrawler:
    def __init__(self, batch_save_size=100):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })
        self.brand_shop_mapping = {}
        self.reviews = []
        self.batch_save_size = batch_save_size  # ì¤‘ê°„ì €ì¥ ë‹¨ìœ„
        self.batch_counter = 0  # ë°°ì¹˜ ì¹´ìš´í„°
        self.total_saved_reviews = 0  # ì´ ì €ì¥ëœ ë¦¬ë·° ìˆ˜
        self.output_file = None  # ë‹¨ì¼ ì¶œë ¥ íŒŒì¼
        self.is_first_save = True  # ì²« ë²ˆì§¸ ì €ì¥ ì—¬ë¶€
    
    def extract_brand_shop_no(self, url):
        """URLì—ì„œ brandShopNo ì¶”ì¶œ"""
        try:
            parsed = urlparse(url)
            params = parse_qs(parsed.query)
            if 'brandShopNo' in params:
                return params['brandShopNo'][0]
            return None
        except Exception:
            return None
    
    def load_brand_mapping(self):
        """categories.csvì—ì„œ ë¸Œëœë“œë³„ brandShopNo ë§¤í•‘ ë¡œë“œ"""
        try:
            df = pd.read_csv('categories.csv', encoding='utf-8')
            
            for _, row in df.iterrows():
                brand = row['brand']
                if brand not in self.brand_shop_mapping:
                    brand_shop_no = self.extract_brand_shop_no(row['url'])
                    if brand_shop_no:
                        self.brand_shop_mapping[brand] = brand_shop_no
            
            print(f"ë¸Œëœë“œ ë§¤í•‘ ë¡œë“œ ì™„ë£Œ: {len(self.brand_shop_mapping)} ê°œ ë¸Œëœë“œ")
            
        except Exception as e:
            print(f"ë¸Œëœë“œ ë§¤í•‘ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def extract_rating(self, rate_elem):
        """ë³„ì  ì¶”ì¶œ"""
        try:
            if 'point5' in rate_elem.get('class', []):
                return 5
            elif 'point4' in rate_elem.get('class', []):
                return 4
            elif 'point3' in rate_elem.get('class', []):
                return 3
            elif 'point2' in rate_elem.get('class', []):
                return 2
            elif 'point1' in rate_elem.get('class', []):
                return 1
            else:
                return 0
        except Exception:
            return 0
    
    def extract_review_options(self, pick_opts_elem):
        """êµ¬ë§¤ ì˜µì…˜ ì •ë³´ ì¶”ì¶œ"""
        options = {}
        try:
            spans = pick_opts_elem.find_all('span')
            for span in spans:
                # <em> íƒœê·¸ë“¤ì„ ì°¾ì•„ì„œ íŒŒì‹±
                em_tags = span.find_all('em')
                if len(em_tags) >= 2:
                    # ì²« ë²ˆì§¸ emì´ ë ˆì´ë¸”, ë‘ ë²ˆì§¸ emì´ ê°’
                    label = em_tags[0].get_text(strip=True)
                    value = em_tags[1].get_text(strip=True)
                    
                    if 'ìƒ‰ìƒ:' in label:
                        options['color'] = value
                    elif 'ì‚¬ì´ì¦ˆ:' in label:
                        options['size'] = value
                    elif 'í‰ì†Œ ì‚¬ì´ì¦ˆ:' in label:
                        options['usual_size'] = value
                    elif 'ê¸¸ì´:' in label:
                        options['length'] = value
                else:
                    # fallback: ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ íŒŒì‹±
                    text = span.get_text(strip=True)
                    if 'ìƒ‰ìƒ:' in text:
                        options['color'] = text.split('ìƒ‰ìƒ:')[-1].strip()
                    elif 'ì‚¬ì´ì¦ˆ:' in text:
                        options['size'] = text.split('ì‚¬ì´ì¦ˆ:')[-1].strip()
                    elif 'í‰ì†Œ ì‚¬ì´ì¦ˆ:' in text:
                        options['usual_size'] = text.split('í‰ì†Œ ì‚¬ì´ì¦ˆ:')[-1].strip()
                    elif 'ê¸¸ì´:' in text:
                        options['length'] = text.split('ê¸¸ì´:')[-1].strip()
        except Exception as e:
            print(f"ì˜µì…˜ íŒŒì‹± ì˜¤ë¥˜: {e}")
            pass
        
        return options
    
    def extract_review_images(self, review_photos_elem):
        """ë¦¬ë·° ì´ë¯¸ì§€ URL ì¶”ì¶œ"""
        images = []
        try:
            if review_photos_elem:
                img_tags = review_photos_elem.find_all('img')
                for img in img_tags:
                    img_url = img.get('data-original') or img.get('src')
                    if img_url and 'noImg' not in img_url:
                        images.append(img_url)
        except Exception:
            pass
        
        return images
    
    def save_batch_reviews(self, force_save=False):
        """ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë¦¬ë·° ì¤‘ê°„ì €ì¥ (ë‹¨ì¼ íŒŒì¼ì— ì¶”ê°€)"""
        if len(self.reviews) < self.batch_save_size and not force_save:
            return
        
        if not self.reviews:
            return
        
        try:
            # ì²« ë²ˆì§¸ ì €ì¥ì‹œ íŒŒì¼ëª… ìƒì„±
            if self.output_file is None:
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                self.output_file = f"reviews_final_{timestamp}.csv"
            
            self.batch_counter += 1
            result_df = pd.DataFrame(self.reviews)
            
            # ì²« ë²ˆì§¸ ì €ì¥ì‹œì—ëŠ” í—¤ë”ì™€ í•¨ê»˜ ìƒˆ íŒŒì¼ ìƒì„±
            if self.is_first_save:
                result_df.to_csv(self.output_file, index=False, encoding='utf-8-sig', mode='w')
                self.is_first_save = False
                print(f"âœ… ìƒˆ íŒŒì¼ ìƒì„±: {self.output_file}")
            else:
                # ì´í›„ì—ëŠ” í—¤ë” ì—†ì´ append ëª¨ë“œë¡œ ì¶”ê°€
                result_df.to_csv(self.output_file, index=False, encoding='utf-8-sig', mode='a', header=False)
            
            self.total_saved_reviews += len(self.reviews)
            
            print(f"âœ… ë°°ì¹˜ {self.batch_counter} ì €ì¥ ì™„ë£Œ: {len(self.reviews)}ê°œ ë¦¬ë·° ì¶”ê°€")
            print(f"   íŒŒì¼: {self.output_file}")
            print(f"   ì´ ì €ì¥ëœ ë¦¬ë·°: {self.total_saved_reviews}ê°œ")
            
            # ë©”ëª¨ë¦¬ì—ì„œ ë¦¬ë·° ë°ì´í„° ì œê±°
            self.reviews = []
            
        except Exception as e:
            print(f"âŒ ë°°ì¹˜ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def merge_batch_files(self):
        """ë‹¨ì¼ íŒŒì¼ì´ë¯€ë¡œ ë³‘í•© ë¶ˆí•„ìš” - ìµœì¢… í†µê³„ë§Œ ì¶œë ¥"""
        if not self.output_file:
            print("ì €ì¥ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        try:
            # ìµœì¢… íŒŒì¼ í†µê³„ ì¶œë ¥
            final_df = pd.read_csv(self.output_file, encoding='utf-8-sig')
            print(f"âœ… ìµœì¢… íŒŒì¼: {self.output_file} (ì´ {len(final_df)}ê°œ ë¦¬ë·°)")
            return self.output_file
            
        except Exception as e:
            print(f"âŒ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
            return self.output_file
    
    def cleanup_batch_files(self):
        """ë‹¨ì¼ íŒŒì¼ì´ë¯€ë¡œ ì •ë¦¬ ë¶ˆí•„ìš”"""
        print("âœ… ë‹¨ì¼ íŒŒì¼ë¡œ ì €ì¥ë˜ì–´ ì •ë¦¬ ê³¼ì • ìƒëµ")
        pass
    
    def get_max_review_pages(self, soup):
        """ë¦¬ë·° ìµœëŒ€ í˜ì´ì§€ ìˆ˜ í™•ì¸"""
        try:
            page_div = soup.find('div', class_='page')
            print(f"ğŸ” í˜ì´ì§€ë„¤ì´ì…˜ div ë°œê²¬: {page_div is not None}")
            
            if page_div:
                # ëª¨ë“  í˜ì´ì§€ ë§í¬ ì°¾ê¸°
                page_links = page_div.find_all('a')
                max_page = 1
                print(f"   ğŸ“„ í˜ì´ì§€ ë§í¬ ê°œìˆ˜: {len(page_links)}")
                
                for link in page_links:
                    onclick = link.get('onclick', '')
                    text = link.get_text(strip=True)
                    class_list = link.get('class', [])
                    
                    print(f"   ğŸ”— ë§í¬ ë¶„ì„: text='{text}', onclick='{onclick}', class={class_list}")
                    
                    if 'getReviewList' in onclick:
                        # onclick="javascript:getReviewList('3','','','');" ì—ì„œ í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ
                        import re
                        match = re.search(r"getReviewList\('(\d+)'", onclick)
                        if match:
                            page_num = int(match.group(1))
                            max_page = max(max_page, page_num)
                            print(f"   âœ“ onClickì—ì„œ í˜ì´ì§€ {page_num} ë°œê²¬")
                    
                    # í˜„ì¬ í˜ì´ì§€ (class="on"ì¸ ê²½ìš°)
                    if 'on' in class_list and text.isdigit():
                        page_num = int(text)
                        max_page = max(max_page, page_num)
                        print(f"   âœ“ í˜„ì¬ í˜ì´ì§€(class=on)ì—ì„œ í˜ì´ì§€ {page_num} ë°œê²¬")
                    
                    # í…ìŠ¤íŠ¸ë¡œ ëœ í˜ì´ì§€ ë²ˆí˜¸ë„ í™•ì¸ (onclickì´ ì—†ëŠ” ê²½ìš°)
                    elif text.isdigit() and not onclick:
                        page_num = int(text)
                        max_page = max(max_page, page_num)
                        print(f"   âœ“ í…ìŠ¤íŠ¸ì—ì„œ í˜ì´ì§€ {page_num} ë°œê²¬")
                
                print(f"   ğŸ“Š ìµœëŒ€ í˜ì´ì§€ ìˆ˜: {max_page}")
                return max_page
            else:
                print("   âŒ í˜ì´ì§€ë„¤ì´ì…˜ divë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            
            return 1
        except Exception as e:
            print(f"   âš ï¸ í˜ì´ì§€ ìˆ˜ í™•ì¸ ì˜¤ë¥˜: {e}")
            return 1
    
    def get_product_reviews(self, product_no, brand, max_pages=50):
        """ìƒí’ˆ ë¦¬ë·° ì¡°íšŒ (ë¦¬ë·°ê°€ ì—†ì„ ë•Œê¹Œì§€ ëª¨ë“  í˜ì´ì§€ í¬ë¡¤ë§)"""
        try:
            # ë¸Œëœë“œë³„ brandShopNo ê°€ì ¸ì˜¤ê¸°
            brand_shop_no = self.brand_shop_mapping.get(brand)
            if not brand_shop_no:
                print(f"ë¸Œëœë“œ {brand}ì˜ brandShopNoë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return []
            
            all_reviews = []
            
            print(f"ğŸ¯ ìƒí’ˆ {product_no}: ë¦¬ë·°ê°€ ì—†ì„ ë•Œê¹Œì§€ ëª¨ë“  í˜ì´ì§€ í¬ë¡¤ë§ (ìµœëŒ€ {max_pages}í˜ì´ì§€ ì•ˆì „ì¥ì¹˜)")
            
            # 1í˜ì´ì§€ë¶€í„° ë¦¬ë·°ê°€ ì—†ì„ ë•Œê¹Œì§€ ê³„ì† í¬ë¡¤ë§
            page = 1
            while page <= max_pages:
                try:
                    print(f"\nğŸ“„ í˜ì´ì§€ {page} í¬ë¡¤ë§ ì¤‘...")
                    
                    # ë¦¬ë·° API URL ìƒì„±
                    url = f"https://www.ssfshop.com/public/goods/detail/listReview?pageNo={page}&sortFlag=&godNo={product_no}&selectAllYn=Y"
                    print(f"   ğŸ”— ìš”ì²­ URL: {url}")
                    
                    response = self.session.get(url, timeout=30)
                    response.raise_for_status()
                    print(f"   âœ… HTTP ì‘ë‹µ: {response.status_code}")
                    print(f"   ğŸ“Š ì‘ë‹µ í¬ê¸°: {len(response.content)} bytes")
                    
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # ë¦¬ë·° ë¦¬ìŠ¤íŠ¸ ì°¾ê¸° (ë” ì •í™•í•œ ì„ íƒì ì‚¬ìš©)
                    review_container = soup.find('div', {'id': 'searchGoodsReviewList'}) or \
                                     soup.find('div', {'class': 'review-detail-lists'})
                    
                    review_list = None
                    if review_container:
                        review_list = review_container.find('ul')
                        print(f"   ğŸ“ ì»¨í…Œì´ë„ˆ ë‚´ ul íƒœê·¸ ë°œê²¬: {review_list is not None}")
                    else:
                        # fallback: ì¼ë°˜ì ì¸ ul íƒœê·¸
                        review_list = soup.find('ul')
                        print(f"   ğŸ“ fallback ul íƒœê·¸ ë°œê²¬: {review_list is not None}")
                    
                    if not review_list:
                        print(f"   âŒ í˜ì´ì§€ {page}: ë¦¬ë·° ë¦¬ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ - í¬ë¡¤ë§ ì¢…ë£Œ")
                        break
                    
                    # ê° ë¦¬ë·° ì•„ì´í…œ ì¶”ì¶œ
                    review_items = review_list.find_all('li')
                    print(f"   ğŸ“ í˜ì´ì§€ {page}: {len(review_items)}ê°œ ë¦¬ë·° ì•„ì´í…œ ë°œê²¬")
                    
                    if not review_items:
                        print(f"   âœ… í˜ì´ì§€ {page}: ë¦¬ë·°ê°€ ì—†ìŒ - ëª¨ë“  ë¦¬ë·° ìˆ˜ì§‘ ì™„ë£Œ!")
                        break
                    
                    page_reviews = []
                    for idx, item in enumerate(review_items):
                        try:
                            review = {
                                'product_no': product_no,
                                'brand': brand,
                                'crawled_at': datetime.now().isoformat(),
                            }
                            
                            # ë¦¬ë·° IDì™€ ì‹œí€€ìŠ¤ (ìƒˆë¡œìš´ êµ¬ì¡°ì— ë§ê²Œ)
                            review['review_id'] = item.get('id', '')
                            review['data_seq'] = item.get('data-seq', '')
                            review['data_godno'] = item.get('data-godno', '')  # ìƒˆë¡œìš´ ì†ì„± ì¶”ê°€
                            
                            # êµ¬ë§¤ ë°©ì‹ (ì˜¨ë¼ì¸/ì˜¤í”„ë¼ì¸)
                            inflow_cd = item.get('data-inflow-ord-sect-cd', '')
                            review['purchase_type'] = 'online' if inflow_cd == 'ONLNE_ORD' else 'offline'
                            
                            # ë””ë²„ê¹…: ì²« ë²ˆì§¸ ë¦¬ë·° ì•„ì´í…œì˜ ìƒì„¸ ì •ë³´ ì¶œë ¥
                            if idx == 0 and page == 1:
                                print(f"   ğŸ” ì²« ë²ˆì§¸ ë¦¬ë·° ìƒ˜í”Œ:")
                                print(f"      ID: {review['review_id']}")
                                print(f"      Data-seq: {review['data_seq']}")
                                print(f"      Data-godno: {review['data_godno']}")
                                print(f"      Purchase type: {review['purchase_type']}")
                            
                            # ë³„ì 
                            rate_elem = item.find('span', class_='rate')
                            review['rating'] = self.extract_rating(rate_elem) if rate_elem else 0
                            
                            # ë§¤ì¥êµ¬ë§¤ ì—¬ë¶€
                            badge_elem = item.find('div', class_='badge')
                            review['is_store_purchase'] = badge_elem is not None
                            
                            # êµ¬ë§¤ ì˜µì…˜
                            pick_opts_elem = item.find('div', class_='pick-opts')
                            if pick_opts_elem:
                                options = self.extract_review_options(pick_opts_elem)
                                review['color'] = options.get('color', '')
                                review['size'] = options.get('size', '')
                                review['usual_size'] = options.get('usual_size', '')
                                review['length'] = options.get('length', '')
                            else:
                                review['color'] = ''
                                review['size'] = ''
                                review['usual_size'] = ''
                                review['length'] = ''
                            
                            # ë¦¬ë·° ë‚´ìš©
                            review_txts_elem = item.find('p', class_='review-txts')
                            if review_txts_elem:
                                review_text = review_txts_elem.get_text(strip=True)
                                # <br> íƒœê·¸ë¥¼ ì¤„ë°”ê¿ˆìœ¼ë¡œ ë³€í™˜
                                review_text = review_text.replace('\n', ' ').strip()
                                review['review_content'] = review_text
                            else:
                                review['review_content'] = ''
                            
                            # êµ¬ë§¤ì ID
                            list_id_elem = item.find('span', class_='list-id')
                            review['reviewer_id'] = list_id_elem.get_text(strip=True) if list_id_elem else ''
                            
                            # ë¦¬ë·° ë‚ ì§œ
                            list_date_elem = item.find('span', class_='list-date')
                            review['review_date'] = list_date_elem.get_text(strip=True) if list_date_elem else ''
                            
                            # ë¦¬ë·° ì´ë¯¸ì§€
                            review_photos_elem = item.find('div', class_='review-photos')
                            images = self.extract_review_images(review_photos_elem)
                            review['review_images'] = '|'.join(images) if images else ''
                            review['has_images'] = len(images) > 0
                            
                            page_reviews.append(review)
                            
                        except Exception as e:
                            print(f"   âš ï¸ ë¦¬ë·° íŒŒì‹± ì˜¤ë¥˜ (í˜ì´ì§€ {page}, ì•„ì´í…œ {idx+1}): {e}")
                            continue
                    
                    all_reviews.extend(page_reviews)
                    print(f"   âœ… í˜ì´ì§€ {page}: {len(page_reviews)}ê°œ ë¦¬ë·° ìˆ˜ì§‘ ì™„ë£Œ")
                    
                    # í˜ì´ì§€ ê°„ ë”œë ˆì´
                    time.sleep(0.5)
                    
                    # ìˆ˜ì§‘ëœ ë¦¬ë·°ê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ
                    if not page_reviews:
                        print(f"   âœ… í˜ì´ì§€ {page}: ìˆ˜ì§‘ëœ ë¦¬ë·°ê°€ ì—†ìŒ - ëª¨ë“  ë¦¬ë·° ìˆ˜ì§‘ ì™„ë£Œ!")
                        break
                    
                    # ë‹¤ìŒ í˜ì´ì§€ë¡œ
                    page += 1
                
                except Exception as e:
                    print(f"   âŒ í˜ì´ì§€ {page} í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
                    # ì—ëŸ¬ê°€ ë‚˜ë„ ë‹¤ìŒ í˜ì´ì§€ ì‹œë„
                    page += 1
                    continue
            
            if page > max_pages:
                print(f"   âš ï¸ ì•ˆì „ì¥ì¹˜ ë„ë‹¬: {max_pages}í˜ì´ì§€ê¹Œì§€ í¬ë¡¤ë§ ì™„ë£Œ")
            
            print(f"ğŸ‰ ìƒí’ˆ {product_no}: ì´ {len(all_reviews)}ê°œ ë¦¬ë·° ìˆ˜ì§‘ ì™„ë£Œ ({page-1}í˜ì´ì§€ê¹Œì§€ í¬ë¡¤ë§)")
            return all_reviews
            
        except Exception as e:
            print(f"âŒ ìƒí’ˆ {product_no} ë¦¬ë·° ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def process_products(self, csv_file, limit=None, max_review_pages=50):
        """products.csv íŒŒì¼ì„ ì½ì–´ì„œ ë¦¬ë·° ìˆ˜ì§‘"""
        try:
            # CSV íŒŒì¼ ì½ê¸°
            df = pd.read_csv(csv_file, encoding='utf-8')
            print(f"ìƒí’ˆ ë°ì´í„° ë¡œë“œ: {len(df)} ê°œ ìƒí’ˆ")
            
            # ì œí•œì´ ìˆìœ¼ë©´ ì ìš©
            if limit:
                df = df.head(limit)
                print(f"í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ {limit}ê°œ ìƒí’ˆë§Œ ì²˜ë¦¬")
            
            total_count = len(df)
            total_reviews = 0
            
            for idx, row in df.iterrows():
                product_no = row.get('product_no', '')
                brand = row.get('brand', '')
                
                if not product_no:
                    print(f"í–‰ {idx + 1}: ìƒí’ˆë²ˆí˜¸ê°€ ì—†ìŒ")
                    continue
                
                print(f"\n[{idx + 1}/{total_count}] {brand} - {product_no} ë¦¬ë·° ìˆ˜ì§‘ ì¤‘...")
                
                # ë¦¬ë·° ê°€ì ¸ì˜¤ê¸°
                reviews = self.get_product_reviews(product_no, brand, max_review_pages)
                self.reviews.extend(reviews)
                total_reviews += len(reviews)
                
                # ë°°ì¹˜ í¬ê¸°ì— ë„ë‹¬í•˜ë©´ ì¤‘ê°„ì €ì¥
                if len(self.reviews) >= self.batch_save_size:
                    self.save_batch_reviews()
                
                # ìƒí’ˆ ê°„ ë”œë ˆì´
                time.sleep(1.0)
                
                # ì§„í–‰ë¥  ì¶œë ¥
                if (idx + 1) % 5 == 0:
                    print(f"ğŸ“Š ì§„í–‰ë¥ : {idx + 1}/{total_count} (í˜„ì¬ ë°°ì¹˜: {len(self.reviews)}ê°œ, ì´ ì €ì¥: {self.total_saved_reviews}ê°œ)")
            
            # ë§ˆì§€ë§‰ ë°°ì¹˜ ì €ì¥ (ë‚¨ì€ ë¦¬ë·°ê°€ ìˆëŠ” ê²½ìš°)
            if self.reviews:
                self.save_batch_reviews(force_save=True)
            
            # ìµœì¢… ê²°ê³¼ ì²˜ë¦¬
            if self.output_file:
                print(f"\nğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
                print(f"ì´ ìƒí’ˆ: {total_count}ê°œ")
                print(f"ì´ ë¦¬ë·°: {self.total_saved_reviews}ê°œ")
                print(f"ê²°ê³¼ íŒŒì¼: {self.output_file}")
                
                # ìµœì¢… íŒŒì¼ í†µê³„ ì¶œë ¥
                final_file = self.merge_batch_files()
                
                if final_file:
                    # ìµœì¢… íŒŒì¼ë¡œ í†µê³„ ì¶œë ¥
                    try:
                        final_df = pd.read_csv(final_file, encoding='utf-8-sig')
                        brand_counts = final_df['brand'].value_counts()
                        print(f"\nğŸ“ˆ ë¸Œëœë“œë³„ ë¦¬ë·° ìˆ˜:")
                        for brand, count in brand_counts.head(10).items():
                            print(f"  - {brand}: {count}ê°œ")
                    except Exception as e:
                        print(f"í†µê³„ ì¶œë ¥ ì‹¤íŒ¨: {e}")
                        
            else:
                print("ìˆ˜ì§‘ëœ ë¦¬ë·°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            print(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def run(self, csv_file=None, limit=None, max_review_pages=50):
        """ë©”ì¸ ì‹¤í–‰"""
        # ë¸Œëœë“œ ë§¤í•‘ ë¡œë“œ
        self.load_brand_mapping()
        
        if not self.brand_shop_mapping:
            print("ë¸Œëœë“œ ë§¤í•‘ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # CSV íŒŒì¼ ì°¾ê¸°
        if not csv_file:
            # ìµœì‹  products_*.csv íŒŒì¼ ì°¾ê¸°
            import glob
            import os
            
            pattern = "products*.csv"
            files = glob.glob(pattern)
            
            if not files:
                print("products*.csv íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                print("ë¨¼ì € ìƒí’ˆ ë¦¬ìŠ¤íŠ¸ í¬ë¡¤ë§ì„ ì‹¤í–‰í•˜ì„¸ìš”: python crawler.py")
                return
            
            # ê°€ì¥ ìµœì‹  íŒŒì¼ ì„ íƒ
            csv_file = max(files, key=os.path.getctime)
            print(f"ìµœì‹  íŒŒì¼ ì„ íƒ: {csv_file}")
        
        # ë¦¬ë·° ìˆ˜ì§‘
        self.process_products(csv_file, limit, max_review_pages)


def main():
    csv_file = None
    limit = None
    max_review_pages = 100  # ì•ˆì „ì¥ì¹˜: ìµœëŒ€ 50í˜ì´ì§€ê¹Œì§€ (ë¦¬ë·°ê°€ ì—†ìœ¼ë©´ ìë™ ì¤‘ë‹¨)
    batch_save_size = 100  # ê¸°ë³¸ ë°°ì¹˜ í¬ê¸°
    
    # ëª…ë ¹í–‰ ì¸ìˆ˜ ì²˜ë¦¬
    if len(sys.argv) > 1:
        args = sys.argv[1:]
        
        i = 0
        while i < len(args):
            arg = args[i]
            
            if arg == '--batch' or arg == '-b':
                # ë‹¤ìŒ ì¸ìˆ˜ê°€ ë°°ì¹˜ í¬ê¸°
                if i + 1 < len(args) and args[i + 1].isdigit():
                    batch_save_size = int(args[i + 1])
                    print(f"ë°°ì¹˜ ì €ì¥ í¬ê¸°: {batch_save_size}ê°œ")
                    i += 1  # ë‹¤ìŒ ì¸ìˆ˜ ê±´ë„ˆë›°ê¸°
                i += 1
                continue
            elif arg.startswith('--batch='):
                batch_save_size = int(arg.split('=')[1])
                print(f"ë°°ì¹˜ ì €ì¥ í¬ê¸°: {batch_save_size}ê°œ")
            elif arg.isdigit():
                # ìˆ«ìëŠ” ëª¨ë‘ ìƒí’ˆ ê°œìˆ˜ ì œí•œìœ¼ë¡œ ì²˜ë¦¬
                limit = int(arg)
                print(f"í…ŒìŠ¤íŠ¸ ëª¨ë“œ: {limit}ê°œ ìƒí’ˆë§Œ ì²˜ë¦¬")
            elif arg.endswith('.csv'):
                csv_file = arg
                print(f"ì§€ì •ëœ íŒŒì¼: {csv_file}")
            
            i += 1
    
    # í¬ë¡¤ëŸ¬ ìƒì„± (ë°°ì¹˜ í¬ê¸° ì„¤ì •)
    crawler = ReviewCrawler(batch_save_size=batch_save_size)
    
    print(f"\nâš™ï¸  ì„¤ì •:")
    print(f"   ë°°ì¹˜ ì €ì¥ í¬ê¸°: {batch_save_size}ê°œ ë¦¬ë·°ë§ˆë‹¤ ì¤‘ê°„ì €ì¥")
    print(f"   ë¦¬ë·° í¬ë¡¤ë§: ë¦¬ë·°ê°€ ì—†ì„ ë•Œê¹Œì§€ ëª¨ë“  í˜ì´ì§€ ìˆ˜ì§‘ (ìµœëŒ€ {max_review_pages}í˜ì´ì§€ ì•ˆì „ì¥ì¹˜)")
    if limit:
        print(f"   í…ŒìŠ¤íŠ¸ ëª¨ë“œ: {limit}ê°œ ìƒí’ˆë§Œ ì²˜ë¦¬")
    print()
    
    crawler.run(csv_file, limit, max_review_pages)


if __name__ == "__main__":
    main() 